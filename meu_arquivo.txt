
Generative AI: 
Perspectives from 
Stanford HAI
How do you think generative AI will affect 
your field and society going forward?

March 2023



Table of Contents 

Introduction	  3

AI’s Great Inflection Point, Fei-Fei Li	  4

The Potentials of Synthetic Patients, Russ Altman	  6

Upending Healthcare, from Patient Care to Billing, Curt Langlotz	  7	

An AI Window into Nature, Surya Ganguli	  8

The New Tools of Daily Life, James Landay	 10

Poetry Will Not Optimize: Creativity in the Age of AI, Michele Elam	 11

Generative AI and the Rule of Law, Daniel E. Ho	 13

The New Cambrian Era: ‘Scientific Excitement, Anxiety’, Percy Liang	 15

A Call to Augment – Not Automate – Workers, Erik Brynjolfsson	 16

The Reinvention of Work, Christopher D. Manning	 18

In Education, a ‘Disaster in the Making’, Rob Reich	 20

Solving Inequalities in the Education System, Peter Norvig	 21



3

Introduction: 
The current wave of generative AI is a subset of artificial intelligence that, 
based on a textual prompt, generates novel content. ChatGPT might write 
an essay, Midjourney could create beautiful illustrations, or MusicLM could 
compose a jingle. Most modern generative AI is powered by foundation 
models, or AI models trained on broad data using self-supervision at scale, 
then adapted to a wide range of downstream tasks.

The opportunities these models present for our lives, our communities, and 
our society are vast, as are the risks they pose. While on the one hand, they 
may seamlessly complement human labor, making us more productive and 
creative, on the other, they could amplify the bias we already experience or 
undermine our trust of information. 

We believe that interdisciplinary collaboration is essential in ensuring these 
technologies benefit us all. The following are perspectives from Stanford 
leaders in medicine, science, engineering, humanities, and the social sciences 
on how generative AI might affect their fields and our world. Some study 
the impact of technology on society, others study how to best apply these 
technologies to advance their field, and others have developed the technical 
principles of the algorithms that underlie foundation models. 

https://crfm.stanford.edu/report.html
https://crfm.stanford.edu/report.html


4

Generative AI: Perspectives  
from Stanford HAI

situations, generative AI can create simulated versions 
of this rare data to further train the AI models or the 
healthcare providers themselves. 

Additionally, before we even start developing 
new generative tools, we need to focus on what 
people want from these tools. In a recent project to 
benchmark robotics tasks by our lab, before even 
starting the research, the project team did a large-
scale user study to ask people how much they would 
benefit if a robot did these certain tasks for them.  
The winning tasks were the focus of the research.

To fully realize the significant opportunity that 
generative AI creates, we need to also evaluate the 
associated risks. Joy Buolamwini led a study titled 
“Gender Shades,” which found AI systems frequently 
fail to recognize women and people of color. Study 
results were published in 2018. We continue to see 
similar bias in generative AI models, specifically for 
underrepresented populations.

AI’s Great Inflection Point

540 million years ago, the number of animal species 
exploded in a very short time period. There are many 
theories as to what happened, but one has captured 
my attention: the sudden onset and ensuing evolution 
of vision. Today, visual perception is a major sensory 
system and the human mind can recognize patterns 
in the world and generate models or concepts based 
on these patterns. Endowing machines with these 
capabilities, generative capabilities, has been a dream 
for many generations of AI scientists. There is a long 
history of algorithmic attempts at generative models 
with varying degrees of progress. In 1966, researchers 
at MIT developed the “Summer Vision Project” to 
effectively construct “a significant part of the visual 
system” with technology. This was the beginning of the 
field of computer vision and image generation.

Recently, due to the profound and interconnected 
concepts of deep learning and large data, we seem 
to have reached an inflection point in the ability of 
machines to generate language, image, audio, and 
more. While building AI to see what humans can see 
was the inspiration for computer vision, we should 
now be looking beyond this to building AI to see 
what humans can’t see. How can we use generative 
AI to augment our vision? Though the exact figure is 
disputed, deaths due to medical error in the U.S. is 
a significant problem. Generative AI models could 
assist healthcare providers in seeing potential issues 
that they may have otherwise missed. Furthermore, 
if the mistakes are due to minimal exposure to rare 

Fei-Fei Li, Sequoia Capital 
Professor in the Computer 
Science Department; Denning 
Co-Director of Stanford HAI

Endowing machines with 
these capabilities, generative 

capabilities, has been a 
dream for many generations 

of AI scientists.

https://www.media.mit.edu/projects/gender-shades/overview/
https://www.bmj.com/content/353/bmj.i2139.long


5

Generative AI: Perspectives  
from Stanford HAI

The ability to determine whether an image was 
generated using AI is also essential. Our society is built 
on trust of citizenship and information. If we cannot 
easily determine whether an image is AI generated, 
our trust of any information will erode. In this case, we 
need to pay special attention to vulnerable populations 
that may be particularly susceptible to adversarial uses 
of this technology.

The progress in a machine’s capability to generate 
content is very exciting, as is the potential to explore 
AI’s ability to see what humans are not able. But 
we need to be attentive to the ways in which these 
capabilities will disrupt our everyday lives, our 
communities, and our role as world citizens.

AI’s Great Inflection Point (contd)



6

Generative AI: Perspectives  
from Stanford HAI

The Potentials of Synthetic Patients

It is often difficult to get large numbers of patients in 
clinical trials and it is crucial to have a realistic group 
of patients who do not receive a therapy in order to 
compare outcomes with those who do. This is one area 
within biomedical research where generative AI offers 
great opportunities. Generative AI could make clinical 
trials more efficient by creating “synthetic” control 
patients (i.e., fake patients) using data from real patients 
and their underlying attributes (to be compared with 
the patients who receive the new therapy). It could even 
generate synthetic outcomes to describe what happens 
to these patients if they are untreated. Biomedical 
researchers could then use the outcomes of real patients 
exposed to a new drug with the synthetic statistical 
outcomes for the synthetic patients. This could make 
trials potentially smaller, faster, and less expensive, and 
thus lead to faster progress in delivering new drugs and 
diagnostics to clinicians and their patients.

In the past, we have used “historical controls” which are 
patients who did not have the benefit of the new drug or 
diagnostic – and compared their outcomes to patients 
who received the new drug or diagnostic. Synthetic 
patients could match the real patients more realistically; 
they are created using knowledge of current 
medications, diagnostic tools, and standards of practice 
that were likely different in the historical situation. 

In the setting of medical education, generative AI could 
allow us to create patients that are very realistic and 
could allow medical students to learn how to detect 

diseases. The ability for generative models to create 
many variations on a theme could allow students to see 
multiple cases of the same disease and learn the ways 
in which these patients can vary. This could give them 
more experience in seeing a disease and provide a 
nearly unlimited set of cases for them to practice if they 
find that certain diseases are more challenging for them 
to recognize and diagnose. These same generative 
models could also interact with the students and give 
them practice eliciting signs and symptoms through 
conversational interaction. 

With opportunity comes worry. If synthetic patients 
are generated from data that does not reflect the 
population of patients receiving the drug, the patients 
may be biased. More worrisome, however, is that even 
the real patients receiving the drug will not reflect the 
full population, and so synthetic controls could just 
improve the use of the drugs for a subset of patients and 
not all – leading to inequity. 

While generative technologies can be very useful in 
accelerating scientific discovery and progress, care 
must be taken in selecting the data used to generate 
patients and the models must be examined very 
carefully for biases that may lead to disparate impact.

Russ Altman, Kenneth Fong Professor 
in the School of Engineering; Professor 
of Bioengineering, of Genetics, of 
Medicine, and of Biomedical Data 
Science; Associate Director of 
Stanford HAI

This could make trials 
potentially smaller, faster, and 
less expensive, and thus lead 

to faster progress in delivering 
new drugs and diagnostics.



7

Generative AI: Perspectives  
from Stanford HAI

Upending Healthcare, from Patient Care to Billing

One of the benefits of our healthcare system is that 
patients can see a variety of specialist physicians 
who are experts in specific medical disciplines. The 
downside of our system is that these specialists often 
aren’t acquainted with the patients they are seeing. 
Imagine a world in which a specialist you are seeing for 
the first time has already read a succinct summary of 
your healthcare needs, created by generative AI. During 
the patient visit, a chatbot based on a foundation model 
could serve as the physician’s assistant to support more 
accurate diagnosis and tailored therapy selection. A 
generative model could draft a clinic note in real time 
based on the physician-patient interaction, leaving more 
time for face-to-face discussion. In the back office, 
generative models could optimize clinic scheduling or 
simplify generation of medical codes for billing, disease 
surveillance, and automated follow-up reminders. 
These new capabilities could improve the accuracy 
and efficiency of patient care while increasing patient 
engagement and adherence to therapy. 

Recent federal legislation gives patients the right to 
access their entire medical record in digital form. As a 
result, patients are increasingly encountering complex 
clinical documents that contain obscure medical 
terms. When a patient returns home from a clinic visit, 
a foundation model could generate tailored patient 
education materials and explain their care plan at the 
appropriate reading level.

Machine learning models in medicine are critically 

dependent on large medical datasets that contain 
examples of disease. We have shown how diffusion 
models, a type of foundation model, can be modified 
to create realistic clinical images from text prompts. 
Our results demonstrate that synthetic training data 
produced by these models can augment real training 
data to increase diagnostic accuracy. This form of 
synthetic data could help solve machine learning 
problems for which training data is scarce, such as the 
detection and treatment of uncommon diseases.

Finally, generative AI’s well-reported challenges 
with factual correctness are particularly problematic 
in medicine, where inaccuracies can cause serious 
harm. Recent problems in medicine include incorrect 
differential diagnosis and invalid scientific citations. 
We are working to improve the factual correctness of 
medical explanations from these models so they can 
achieve an accuracy that is suitable for safe clinical use.

Curt Langlotz, Professor of Radiology, of Biomedical Informatics Research, and 
of Biomedical Data Science; Director of the Center for Artificial Intelligence in 
Medicine and Imaging (AIMI); Associate Director of Stanford HAI

During the patient visit, a 
chatbot … could serve as the 

physician’s assistant to support 
more accurate diagnosis and 

tailored therapy selection.

https://arxiv.org/abs/2211.12737
https://arxiv.org/abs/2010.10042


8

Generative AI: Perspectives  
from Stanford HAI

An AI Window into Nature

Scientific ideas from the study of nature itself, in the 
form of nonequilibrium thermodynamics and the 
reversal of the flow of time, lead to the creation at 
Stanford of the first diffusion model, a key kernel of 
technology that forms the basis of many successful 
AI generative models today. Now, in a virtuous 
cycle, AI generative models are well poised to 
deliver considerable insights into nature itself, across 
biological, physical, and mental realms, with broad 
implications for solving key societal problems.

For example, generative models of proteins can allow 
us to efficiently explore the space of complex three-
dimensional protein structures, thereby aiding in the 
search for proteins with novel and useful functions, 
including new efficacious medicines. Generative 
AI is starting to be explored in the quantum realm, 
enabling us to efficiently model strongly correlated 
states of electrons, with the potential of advancing 
our understanding of materials science and quantum 
chemistry. These advances could in turn lead to 
the creation of new materials and catalysts that 
could play a role in efficient energy capture and 
storage. Simple generative modeling, intertwined 
with classical numerical solvers, has also made 
key advances in accurate and fast large scale fluid 
mechanical simulations, which when scaled up, could 
aid in climate modeling and weather forecasting, 
thereby contributing to a deeper understanding of our 
changing climate and its ramifications. 

In a beautiful recursion, the generative AI models that 
we have created can also act as scientific windows, 
not only into the physical world but also into our 
own minds. For the first time, we have AI systems 
that can model high-level cognitive phenomena 
like natural language and image understanding. 
Many neuroscientists and cognitive scientists have 
compared the neural representations of both deep 
networks and AI generative models to neurobiological 
representations in humans and animals, often finding 
striking similarities across many brain areas. Examples 
include the retina, the ventral visual stream, motor 
cortex, entorhinal cortex for navigation, cortical 
language areas, and neural geometries underlying few 
shot concept learning. The often similar structure of 
artificial and biological solutions to generative tasks 
suggests there may be some common principles 
governing how intelligent systems, whether biological 
or artificial, model and generate complex data.

Surya Ganguli, Associate 
Professor of Applied Physics; 
Associate Director of 
Stanford HAI 

AI generative models are well 
poised to deliver considerable 

insights into nature itself, 
across biological, physical, 

and mental realms, with 
broad implications for solving 

key societal problems.

https://proceedings.mlr.press/v37/sohl-dickstein15.html
https://www.biorxiv.org/content/10.1101/2022.12.09.519842v2
https://arxiv.org/abs/2204.12966
https://pubs.acs.org/doi/10.1021/acsomega.2c03264
https://www.science.org/stoken/author-tokens/ST-218/full
https://www.science.org/stoken/author-tokens/ST-218/full
https://www.annualreviews.org/doi/10.1146/annurev-fluid-010719-060214
https://www.annualreviews.org/doi/10.1146/annurev-fluid-010719-060214
https://hai.stanford.edu/news/intertwined-quest-understanding-biological-intelligence-and-creating-artificial-intelligence
https://proceedings.neurips.cc/paper/2019/hash/eeaebbffb5d29ff62799637fc51adb7b-Abstract.html
https://www.pnas.org/doi/10.1073/pnas.2014196118
https://www.nature.com/articles/nn.4042
https://www.nature.com/articles/nn.4042
https://www.cell.com/neuron/pdfExtended/S0896-6273(22)00907-2
https://www.nature.com/articles/s41598-022-20460-9
https://www.nature.com/articles/s41598-022-20460-9
https://www.pnas.org/doi/abs/10.1073/pnas.2200800119
https://www.pnas.org/doi/abs/10.1073/pnas.2200800119


9

Generative AI: Perspectives  
from Stanford HAI

An exceedingly interesting and profound question 
arises in the forthcoming age of scientific collaboration 
between humans and AI systems as they work 
together in a loop to analyze our complex biological, 
physical, and mental worlds: What does it mean for 
a human to derive an interpretable understanding of 
a complex system when an AI provides a substantial 
part of that understanding through predictive models? 
Issues regarding explainable AI will likely rise to 
the fore when a fundamentally human scientific 
endeavor, namely understanding our world, is partially 
achieved through the use of AI. Human scientists will 
not be content with uninterpretable AI-generated 
predictions alone. They will desire human interpretable 
understanding, in addition. 

Finally, to dream even bigger, while today’s generative 
AI has access to immense global scale training data 
spanning images, text, and video from the internet, 
it does not have direct access to our own thoughts, 
in the form of neural activity patterns. However, this 
need not always be the case, given remarkable new 
neuroscientific capacities to record many neurons 
from the brains of animals while they view images, as 
well as to perform MEG, EEG, and fMRI from humans 
as they experience the world through rich multimodal 
sensory experiences. Such combined neural and 
real-world data could then potentially be used to 
train next generation multimodal foundation models 
that not only understand the physical world but also 
understand the direct impact the physical world has 
on our mental world, in terms of elicited neural activity 
patterns. What might such hybrid biological-artificial 
intelligences teach us about ourselves?

Overall, the future of generative AI as a window into 

An AI Window into Nature (contd)

nature, and the use of this window to solve societal 
problems, is full of promise. We certainly do live in 
interesting times.

https://arxiv.org/abs/2006.00093


10

Generative AI: Perspectives  
from Stanford HAI

The New Tools of Daily Life

As we all know, AI is taking the world by storm. We 
will begin to see many new tools that augment our 
abilities in professional and personal activities and 
workflows. Imagine a smart tutor that is always patient 
and understands the level of knowledge the student 
has at any point in time on any subject. These tutors 
will not replace teachers, but instead will augment the 
student learning experience – giving students a more 
personalized interaction, focusing in areas where they 
might be weaker. 

In design, picture a tool that assists a professional 
designer by riffing off their initial design ideas and 
helping them explore more ideas or fill in details on 
their initial ideas. Generative AI will also unleash 
language-based interfaces, whether written or 
spoken, as a more common way of interacting with 
our everyday computing systems, especially when on 
the go or when our eyes and hands are busy. Imagine 
an Alexa, Siri, or Google Assistant that can actually 
understand what you are trying to do rather than just 
answering simple queries about the weather or music.

While generative AI creates many exciting 
opportunities, we know from past AI deployments 
there are risks. In 2016, an AI-based software tool used 
across the country to predict if a criminal defendant 
was likely to reoffend in the future was shown to be 
biased against Black Americans. We need to ensure 
we are designing these tools to get the most positive 
outcomes. To do this, we need to deeply design and 

analyze these systems at the user, the community, and 
societal levels. At the user level, we need to create 
new designs that augment people by accounting for 
their existing workflows and cognitive abilities. But we 
can’t just design for the user. We need to consider the 
community that the system impacts: the families, the 
infrastructure, and the local economy. But, even that is 
not enough, we need to analyze the impacts to society 
at large. We need to be able to forecast what happens 
if the system becomes ubiquitous and from the start 
design mitigations for possible negative impacts.

Our user interface to computing has been fairly static 
over the last 30 years. In the next 5–10 years, we 
will see a revolution in human-computer interaction. 
Changes that are underpinned by generative AI are 
only now starting to be imagined by designers and 
technologists. Now is the time to ensure that we are 
critically thinking about the user, the community, and 
the societal impacts.

James Landay, Anand Rajaraman and Venky Harinarayan 
Professor in the School of Engineering and Professor of 
Computer Science; Vice Director of Stanford HAI

Changes that are underpinned 
by generative AI are only now 

starting to be imagined by 
designers and technologists.



11

Generative AI: Perspectives  
from Stanford HAI

Poetry Will Not Optimize: 
Creativity in the Age of AI

In 2018, the professional art world was upended when 
the renowned Christie’s auction house sold an AI-
augmented work, “Portrait of Edmond Belamy,” for the 
wildly unexpected sum of $435,000. That sale, which 
came with the tacit imprimatur of the established art 
community, generated much gnashing of teeth and 
hand-wringing in the arts sector over what artificial 
intelligence means for the creative industry.

Since then, the genie has long fled its lamp: Generative 
AI has enabled visual art of every known genre as well 
as AI-augmented poetry, fiction, film scripts, music 
and musicals, symphonies, AI-curated art histories, 
and much more.

The furor over the Christie’s sale may now seem 
quaint – it occurred before DALL-E, Lensa AI, 
ChatGPT, Bing, to name just a few – but it heralded 
many of today’s increasingly ferocious debates over 
the nature of creativity and the future of work for the 
creative industry. It anticipated the current hornet’s 
nest of ethical, political, and aesthetic concerns that 
generative AI poses for the arts.

Some of these concerns have been productive: 
Generative AI has encouraged many of those whose 
livelihoods, and in many cases their identities, 
depend on their artistic productions to consider 
anew – and in new ways – perennial questions about 
foundational aesthetic norms and value: What do 

we identify as “art”? What counts as “good” art? Is 
artistry defined by human agency or automation? 
Just who or what can make “art”? And who decides? 
Generative AI raises important, thorny questions about 
authenticity, economic valuation, provenance, creator 
compensation, and copyright. (The Getty Images 
lawsuit against Stable Diffusion is just the tip of an 
iceberg.) It also, arguably, normalizes extractive and 
exploitative approaches to creators and their work; 
amplifies biases of every kind; exacerbates already 
urgent educational and national security concerns 
around deep fakes and plagiarism, especially in the 
absence of congressional regulation.

Perhaps the most pressing concern, in terms of 
national security, is that generative AI might take 
advantage of the fact that the arts have always shaped 
– for good or ill – the civic imagination, that stories, 
films, plays, images shape our perception of ourselves, 
of our physical and social realities. One of the most 
famous disagreements between Plato and his student 
Aristotle was over the potentially dangerous power of 

Michele Elam, William Robertson 
Coe Professor in the School of 
Humanities and Sciences and 
Professor of English; Associate 
Director of Stanford HAI

Should the principles of 
efficiency, speed, and  

so-called blessings of scale 
apply so unequivocally to 

creative processes? After all, 
poetry does not optimize.

https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx
https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx
https://artsexperiments.withgoogle.com/poemportraits?_ga=2.33161846.992826029.1556786810-799000725.1554196893
https://en.wikipedia.org/wiki/Sunspring
https://musically.com/2020/01/27/deepjams-is-latest-project-exploring-original-ai-generated-music
https://www.bing.com/videos/search?q=Can+a+computer+write+a+musical&docid=608050107663257453&mid=8AAC842FC607B2BF1A858AAC842FC607B2BF1A85&view=detail&FORM=VIREhttps://www.nbcnews.com/mach/science/ai-can-now-compose-pop-music-even-symphonies-here-s-ncna1010931
https://musically.com/2020/01/27/deepjams-is-latest-project-exploring-original-ai-generated-music


12

Generative AI: Perspectives  
from Stanford HAI

poesy to influence beliefs and worldviews. This power 
is why fascist regimes first do away with the artists and 
intellectuals: because they hold sway over our minds 
and thus our actions. 

Some claim that generative AI is democratizing access 
to creative expression to those traditionally barred 
from it by lack of status or wealth. But do claims to 
“democratization” and “access” function, in effect, as 
industry cover for rushing a commercial application 
“into the wild” (i.e., to the public) without the time-
intensive work of ensuring ethical guardrails?

Is AI simply a neutral if powerful assistive tool for the 
arts – akin to pen, paintbrush, or photography? Is it 
“blitzscaling” creativity, or in Emad Mostaque’s choice 
description, relieving our “creatively constipated” 
world with AI technologies that can have us all 
“pooping rainbows”? Despite centuries’ worth of 
opining by poets, philosophers, and pundits of all kinds 
about the nature of “creativity,” no settled definition 
exists. Given this, technological claims to expedite that 
so little-understood phenomenon carry more than a 
whiff of hubris.

In fact, generative AI may simply automate a highly 
reductive notion of both the creative process and of 
the learning process itself. Should the principles of 
efficiency, speed, and so-called blessings of scale 
apply so unequivocally to creative processes? After all, 
poetry does not optimize. Fiction is not frictionless.

Consider the slowed-down, recursive reading and 

interpretive skills required to understand any piece 
of writing by Toni Morrison. Her work always invites 
us to pause, insists we reflect. Consider what natural 
language processing applications informing foundation 
models make of African American Vernacular English, 
not to mention Morrison’s signifying on that language 
system. Just try the experiment of my students, who 
submitted an excerpt of Toni Morrison’s Beloved to 
Grammarly, which attempted to correct her exquisite 
prose for what sociolinguists term “standard English,” 
and quickly saw how even deeply rich meaning can be 
rendered impotent.

Historically, creative expression – especially poetry, 
painting, novels, theater, music – has always been 
considered a distinguishing feature of humanity and 
the pinnacle of human achievement. Can generative AI 
live up to that?

Maybe.
Maybe not.
Definitely not yet.

Poetry Will Not Optimize: 
Creativity in the Age of AI (contd)

https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html
https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html
https://www.linkedin.com/pulse/blitzscaling-creativity-dall-e-reid-hoffman/
https://www.nytimes.com/2022/10/21/technology/generative-ai.html
https://www.nytimes.com/2022/10/21/technology/generative-ai.html


13

Generative AI: Perspectives  
from Stanford HAI

Generative AI and the Rule of Law

In January 2023, a Colombian court was faced with 
the question of whether an indigent guardian of an 
autistic minor should be exempted from paying for 
therapy costs. 

It might have been an ordinary case. But the 
judge consulted ChatGPT. The prompt: “Has the 
jurisprudence of the constitutional court made 
favorable decisions in similar cases?” 

While quick to note that ChatGPT was not replacing 
judicial discretion, the judge noted, generative AI 
could “optimize the time spent writing judgments.” 

The Colombian case may be the first judicial 
proceeding incorporating generative AI, and it 
exemplifies both what is promising, but also terrifying, 
about generative AI and the rule of law. 

On the one hand, the United States faces an access 
to justice problem of tragic proportions. In 1978, 
President Carter delivered a speech to the American 
Bar Association, admonishing the profession: “We 
have the heaviest concentration of lawyers on 
earth. … Ninety percent of our lawyers serve 10 
percent of our population. We are overlawyered and 
underrepresented.” (“The situation has not improved,” 
said Deborah Rhode in 2014.) Veterans wait some 
5–7 years for the appeals of disability benefits to be 
decided. The right to counsel with underfunded public 
defenders has turned into a “meet ’em and plead ’em” 

system. And even though the United States yields 
one of the highest per capita rates of lawyers, legal 
representation is out of reach for most. 

Therein lies the promise. Just as legal databases 
such as Westlaw and Lexis revolutionized legal 
research, there is the potential for generative AI to 
help individuals prepare legal documents, attorneys 
in legal research and writing, and judges to improve 
the accuracy and efficiency of painfully slow forms of 
adjudication. While the industrial organization of legal 
search could get in the way, generative AI could help 
level the legal playing field. 

But the Colombian case also illustrates everything 
that can be wrong with the use of generative AI. Such 
models can lie, hallucinate, and make up facts, cases, 
and doctrine. (Insert mandatory joke about lawyers 
lying and cheating too.) Relying on ChatGPT as a 

Daniel E. Ho, William Benjamin Scott and Luna M. Scott Professor in 
Law at Stanford Law School and Director of the Regulation, Evaluation, 
and Governance Lab (RegLab); Associate Director of Stanford HAI

Relying on ChatGPT as a 
substitute for legal research 

poses grave problems for 
professional ethics and, 

ultimately, the rule of law.

https://www.vice.com/en/article/k7bdmv/judge-used-chatgpt-to-make-court-decision
https://legal.thomsonreuters.com/en/westlaw
https://www.lexisnexis.com/en-us/home.page
https://reglab.stanford.edu/
https://reglab.stanford.edu/


14

Generative AI: Perspectives  
from Stanford HAI

substitute for legal research poses grave problems for 
professional ethics and, ultimately, the rule of law. 

Why is that the case? What the law teaches us is that 
justice is as much about the process as the outcome. 
A fair process engenders public trust. And the process 
for embedding generative AI in legal decision-making 
is as important as getting the foundation model right. 
Significant technical research will be required to 
prevent generative AI from making up facts, cases, 
and doctrine. Or better yet: to think like a lawyer. But 
even if that is solved – a big if – we cannot resolve the 
most contentious disputes that are channeled into law 
unless humans trust, participate, buy in, and engage 
in the process. Justice delayed is justice denied, but 
optimizing the time to write judgments is not the right 
objective either. 

Or, as ChatGPT puts it, “Judges should not use 
ChatGPT when ruling on legal cases.” At least not yet. 

Generative AI and the Rule of Law (contd)



15

Generative AI: Perspectives  
from Stanford HAI

The New Cambrian Era: 
‘Scientific Excitement, Anxiety’

For almost all of human history, creating novel artifacts 
(literary works, art, music) was difficult and only 
accessible to experts. But with recent advances in 
foundation models, we are witnessing a Cambrian 
explosion of AI that can create anything from videos to 
proteins to code with uncanny fidelity. This is incredibly 
enabling, lowering the barrier to entry. It is also 
terrifying, as it eliminates our ability to determine what 
is real and what is not, and it will upend the creative 
industry (artists, musicians, programmers, writers). 

Foundation models are based on deep neural networks 
and self-supervised learning which has existed for 
decades; however, the amount of data with which 
these recent models can be trained results in emergent 
abilities, abilities not present when the models were 
trained on less data. In 2021, we released a paper 
detailing the opportunities and risks of foundation 
models. We discuss how these emergent abilities are a 
“source of scientific excitement but also anxiety about 
unanticipated consequences.” Along with emergent 
abilities, we discuss homogenization. In the case of 
foundation models, “the same few models are reused 
as the basis for many applications. This centralization 
allows us to concentrate and amortize our efforts 
(e.g., to improve robustness, to reduce bias) on a 
small collection of models that can be repeatedly 
applied across applications to reap these benefits 
(akin to societal infrastructure), but centralization also 
pinpoints these models as singular points of failure 

that can radiate harms (e.g., security risks, inequities) 
to countless downstream applications.” Understanding 
emergent behavior and homogenization in foundation 
models are just as relevant, if not more, now than just 
two years ago. 

Additionally, it is absolutely critical that we benchmark 
these foundation models to better understand their 
capabilities and limitations as well as use these insights 
to guide policymaking. Toward that end, we recently 
developed HELM (Holistic Evaluation of Language 
Models). HELM benchmarks over 30 prominent 
language models across a wide range of scenarios 
(e.g., question answering, summarization) and for a 
broad range of metrics (e.g., accuracy, robustness, 
fairness, bias, toxicity) to elucidate their capabilities 
and risks. There will continue to be new models and 
associated scenarios and metrics. We welcome the 
community to contribute to HELM. 

Percy Liang, Associate Professor 
of Computer Science; Director of 
Stanford Center for Research on 
Foundation Models

This is incredibly enabling, 
lowering the barrier to entry. 

It is also terrifying, as it 
eliminates our ability  
to determine what is  
real and what is not.

https://crfm.stanford.edu/report.html
https://crfm.stanford.edu/2022/11/17/helm.html


16

Generative AI: Perspectives  
from Stanford HAI

A Call to Augment – Not Automate – Workers

Over the past two decades, most uses of computers, 
including earlier waves of AI, primarily affected 
workers with less education and training. As a result, 
income inequality tended to increase in the U.S. and 
many other developed nations. In contrast, generative 
AI has the potential to affect many types of work that 
have primarily been done by well-compensated people 
including writers, executives, entrepreneurs, scientists, 
and artists. This may reverse some of the past effects 
of IT and AI when it comes to inequality. So far, there 
have been speculation and case examples, but not 
much systematic empirical evidence either way.

At Stanford Digital Economy Lab, we are cataloging 
the list of economic activities likely to be affected 
by generative AI and estimating what share of the 
economy they represent. Generative AI promises to 
automate or augment many of the thousands of tasks 
done in the economy that previously could only be 
done by humans. In particular, writing nonfiction essays, 
persuasive ad copy, intriguing fiction, evocative poetry, 
concise summaries, entertaining lyrics, and other 
forms of text of reasonable quality is an important part 
of many occupations. So is writing code, generating 
images, and creating new designs. This will almost 
surely increase total output, reduce costs, or both. 
Either way, productivity is likely to rise, although some 
of the benefits (and costs) are not well measured.

In cases where generative AI can be a complement 
to labor, particularly for knowledge workers and the 

creative class, wages could increase even as output 
increases. In other cases, the effects may be primarily 
to substitute for labor, as the technology replaces 
workers in some tasks. Likewise, the technology 
can be used to concentrate wealth and power, by 
facilitating winner-take-all dynamics or to decentralize 
and distribute decision-making and economic 
power, by lowering barriers to entry and fixed costs, 
empowering more people to create value. It can 
create a monoculture of closely related output, or a 
flourishing of novel creations.

Last but not least, these technologies have the 
potential to speed up the rate of innovation itself, by 
facilitating invention, design, and creativity. Thus they 
may not only increase the level of productivity but also 
accelerate its rate of change.

Erik Brynjolfsson, Jerry Yang and Akiko Yamazaki Professor 
at Stanford HAI; Director of Stanford Digital Economy Lab

This will almost surely increase 
total output, reduce costs, 

or both…productivity is likely 
to rise, although some of the 
benefits (and costs) are not 

well measured.



17

Generative AI: Perspectives  
from Stanford HAI

Powerful new technologies almost always require 
significant changes in intangibles like business 
organization, process, and skills. Generative AI is not 
likely to be an exception. Given the rapid advances in 
the technology, a growing gap is emerging between 
the technological capabilities and the economic 
complements needed. This will create tensions and 
disruptions, but also opportunities for rapid progress. 
Understanding these tensions and opportunities is 
central to our research agenda.

The effects of generative AI are not necessarily 
predetermined. Instead, they depend on choices by 
technologists, managers, entrepreneurs, policymakers, 
and many others.

A Call to Augment – Not Automate – 
Workers (contd)



18

Generative AI: Perspectives  
from Stanford HAI

Christopher D. Manning, Thomas M. Siebel Professor in Machine Learning 
at the School of Engineering; Professor of Linguistics and of Computer 
Science; Director of Stanford AI Lab; Associate Director of Stanford HAI

The Reinvention of Work

Imagine a business analyst or data scientist generating 
a visualization, say, of how changes in voting patterns 
and economic growth correlate or anti-correlate 
by county in the U.S. over the last decade. At the 
moment, they’ll typically spend a few hours on the 
task: searching to find out where the right data lives, 
writing some SQL or Python code to grab that data, 
then spending more time, perhaps in Tableau, d3, or 
again in Python, to turn it into a nice visualization. 
Maybe by next year, AI will be able to fulfill a long-
standing dream: The business analyst will just be able 
to say, “Generate a heatmap visualization over a U.S. 
map showing the correlation between voting patterns 
and economic growth by county in the U.S. over the 
last decade.” The generative AI system will do the 
job in seconds, and to the extent that the first work 
product isn’t exactly what the person wanted, they’ll 
be able to continue a back-and-forth dialog to refine 
the visualization. 

In our daily world, built by humans for humans, the 
major medium for communication is through human 
language – whether it is speaking with someone in 
person, on the phone, or by Zoom; or communicating 
in written form via anything from texts to emails to 
lengthy reports. Because of this, generative language 
models provide a massive opportunity to reinvent 
how work is done inside all sorts of companies and 
industries: Marketing, sales, product development, 
customer support, and even human resources will all 
change. Recent generative AI models are sufficiently 

good to offer enormous help – and hence potential 
cost savings in a business context. In some cases, 
a large language model–based system might be 
able to take over a whole interaction, working with 
a human being to get things done. There is no doubt 
that a person in marketing and copywriting can get 
significant creative assistance from these models: 
A generative language model can suggest better 
wordings or hip, catchy phrases. Given one sample 
paragraph, it can generate 10 other possibilities, which 
a person might mine the best parts from, or just use 
them all to provide a variety of messages. 

There are many intriguing aspects of this technological 
future that deserve further thought and comment. 
We’re still in the early days of figuring out what new 
models of normal business practice are and aren’t 
possible. In nearly all cases, the AI system will help 
humans to get work done. As such, it continues the 

These AI models are not going 
to provide Toni Morrison–level 
prose nor her lived experience, 
but, I believe, they will produce 

very competent prose.



19

Generative AI: Perspectives  
from Stanford HAI

The Reinvention of Work (contd)

story of new technologies and automation making 
things easier and improving quality of life. Washing 
machines made washing clothes much easier. 

For almost the entire history of civilization, whether 
in the Middle East, Europe, or China, the ability to 
write well has been seen as absolutely central and 
vital to human accomplishment and professionalism, 
something still reflected in the way universities today 
emphasize developing their students’ writing skills. 
We will have to reckon with that changing: As Michele 
Elam notes in her piece, these AI models are not 
going to provide Toni Morrison–level prose nor her 
lived experience, but, I believe, they will produce very 
competent prose. 



20

Generative AI: Perspectives  
from Stanford HAI

In Education, a ‘Disaster in the Making’

The newest revolution in artificial intelligence is powerful 
new automatic writing tools. In professional settings, 
these models can augment human performance – 
rewrite our client emails in a more professional tone, 
complete our papers, or generate a report on our 
company’s annual performance. However, in educational 
settings, absent special design considerations, these 
models could undermine performance and corrode our 
creative abilities. Calculators have proven to promote 
accuracy, remove some of the more tedious work, and 
make math more enjoyable for many. ChatGPT is not 
like a calculator. Why? The quality of your writing is 
not just a measure of your ability to communicate; it is 
a measure of your ability to think. If students lean on 
ChatGPT to write their essays, if they do not learn to 
express their thoughts in writing in a clear, concise, and 
cohesive manner, then their thoughts themselves are not 
clear, concise, or cohesive. The ability to write exercises 
their thinking; learning to write better is inseparable 
from learning to think better. Becoming a good writer 
is the same thing as becoming a good thinker. So if text 
models are doing the writing, then students are not 
learning to think.

Initially, the new wave of generative AI (e.g., GPT, 
DALL-E) was treated with caution and concern. OpenAI, 
the company behind some of these models, restricted 
their external use and did not release the source code 
of its most recent model as it was so worried about 
potential abuse. OpenAI now has a comprehensive 
policy focused on permissible uses and content 
moderation.

But as the race to commercialize the technology has 
kicked off, those responsible precautions have not been 
adopted across the industry. In the past six months, 
easy-to-use commercial versions of these powerful AI 
tools have proliferated, many of them without the barest 
of limits or restrictions.

So how could we prevent this disaster-in-the-making 
in education? First, AI developers and policymakers 
must distinguish between the significance of foundation 
models in educational versus professional settings. Then, 
they must work together, along with industry players, 
to develop community norms. This isn’t new ground. 
Look to bioengineering, where the leading researchers, 
such as Jennifer Doudna, developed norms around 
the appropriate use of CRISPR technology. For AI, that 
would mean ​​companies establishing a shared framework 
for the responsible development, deployment, or release 
of language models to mitigate their harmful effects. 

In an environment where companies are sprinting to 
launch their latest models, we cannot be content to wait 
and see the ethical and societal impact and patch things 
up later. We need to develop widely shared norms now 
before we as a society pay the price.

Rob Reich, Professor of Political 
Science; Director of Stanford McCoy 
Family Center for Ethics in Society; 
Associate Director of Stanford HAI

Calculators have proven to 
promote accuracy, remove some of 
the more tedious work, and make 

math more enjoyable for many. 
ChatGPT is not like a calculator.

https://www.theguardian.com/commentisfree/2022/nov/28/ai-students-essays-cheat-teachers-plagiarism-tech
https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction
https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction
https://hai.stanford.edu/news/time-now-develop-community-norms-release-foundation-models


21

Generative AI: Perspectives  
from Stanford HAI

Solving Inequalities in the Education System

We know that learning is suboptimal when a 
lecturer drones on and on to a large crowd with no 
interaction. And yet, that’s what happens in many 
classrooms. We know that learning is at its best when 
a knowledgeable, inspiring, empathetic tutor works 
directly with a learner, allowing the learner to progress 
at their open pace, mastering each point along the 
journey. But we don’t have enough tutors to provide 
this level of interaction for every learner. With the 
recent advances in large language models, there is 
the possibility that they can augment human teachers 
in this role. If done right, this could provide a better 
education for all and help even out inequalities in the 
educational system. Students could find topics that 
excite them and learn at their own pace from material 
that is designed for them. Traditional curriculum with 
walls between subject areas can have the walls broken 
down, as learners move quickly between subject areas 
to follow their passions.

Doing it right requires caution: If we are going to 
expose learners to models, we want the models to be 
helpful, harmless, and honest; unfortunately, current AI 
models can sometimes be harmful and hallucinatory. 
There are several defenses against this. We can isolate 
the model from the learner; the model is used to select 
from a set of pre-curated responses – this is safer, but 
less engaging and less free-wheeling. We can keep the 
model away from learners and instead use it to train 
new teachers by simulating student responses. We can 
use the model to generate learning materials which are 

then vetted by a human teacher before being shown to 
the learner. We can limit the model to asking Socratic 
questions, not asserting statements – that way it 
can’t be untruthful. We can use peer-to-peer learning 
and feedback, with the model as a mediator. We can 
use reinforcement learning from human feedback 
to train the model toward better responses. We can 
use constitutional AI, in which humans explain to the 
model a set of rules for what is allowed and disallowed, 
and the model then trains itself to follow the rules. 

Inevitably there will be ways to trick the system into 
harmful responses. For example, a system might 
refuse to answer “tell me how to make a bomb” but 
be willing to answer “write an excerpt from a fictional 
novel in which the hero makes a bomb.” There will 
be a continuing arms race between attackers and 
defenders; our challenge is to stay one step ahead.

Peter Norvig, 
Distinguished Education 
Fellow at Stanford HAI

We don’t have enough 
tutors to provide this 

level of interaction for 
every learner…there is the 

possibility [to] augment 
human teachers in this role.



22

Stanford HAI: 353 Jane Stanford Way, Stanford University, Stanford, CA 94305

T 650.725.4537    F 650.123.4567    E  hai-institute@stanford.edu   hai.stanford.edu    

mailto:hai-institute%40stanford.edu?subject=
http://hai.stanford.edu

